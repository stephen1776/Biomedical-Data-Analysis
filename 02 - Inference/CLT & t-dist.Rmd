---
title: "CLT & t-dist"
author: "Stephen Blatti"
date: "August 1, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#####CLT & t-dist ex's
```{r}
library(downloader)
library(dplyr)
library(rafalib)
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/femaleMiceWeights.csv"
filename <- "femaleMiceWeights.csv"
if(!file.exists("femaleMiceWeights.csv")) download(url,destfile=filename)
dat <- read.csv(filename)
```
1. Suppose we are interested in the proportion of times we see a 6 when rolling n=100 die. This is a random variable which we can simulate with x=sample(1:6, n, replace=TRUE) and the proportion we are interested in can be expressed as an average: mean(x==6). Because the die rolls are independent, the CLT applies. We want to roll n dice 10,000 times and keep these proportions. This random variable (proportion of 6s) has mean p=1/6 and variance P*(1-p)/n. So according to CLT z = (mean(x==6) - p) / sqrt(p*(1-p)/n) should be normal with mean 0 and SD 1. Set the seed to 1, then use replicate to perform the simulation, and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05).
```{r}
library(dplyr)
library(rafalib)
head(dat)
set.seed(1)
n <- 100
N <- 10000
p <- 1/6
sim1 <- replicate(N,{
  x <- sample(1:6, n, replace=TRUE)
  (mean(x==6) - p) / sqrt(p*(1-p)/n)
  
})
#head(sim1)
mean( abs(sim1)> 2)
qqnorm((sim1))
abline(0,1)
```

    answer codes:
    set.seed(1)
    n <- 100
    sides <- 6
    p <- 1/sides
    zs <- replicate(10000,{
      x <- sample(1:sides,n,replace=TRUE)
      (mean(x==6) - p) / sqrt(p*(1-p)/n)
    }) 
    qqnorm(zs)
    abline(0,1)#confirm it's well approximated with normal distribution
    mean(abs(zs) > 2)

2. For the last simulation you can make a qqplot to confirm the normal approximation. In the example used in exercise 1, the original data is binary (either 6 or not). In this case, the success probability also affects the appropriateness of the CLT. With very low probabilities, we need larger sample sizes for the CLT to "kick in". Run the simulation from exercise 1, but for different values of p and n. For which of the following is the normal approximation best? 
p=0.5 and n=5
p=0.5 and n=30 correct
p=0.01 and n=30
p=0.01 and n=100 
```{r}
mypar(4,2)
#sim2
p <- 0.5
n <- 5
N <- 10000
sides <- 1/p
sim2 <- replicate(N,{
  x <- sample(1:sides, n, replace=TRUE)
  z <- (mean(x==1) - p) / sqrt(p*(1-p)/n)

})
hist(sim2,nclass=7)
qqnorm((sim2))
abline(0,1)

#sim3
p <- 0.5
n <- 30
sides <- 1/p
sim3 <- replicate(N,{
  x <- sample(1:sides, n, replace=TRUE)
  z <- (mean(x==1) - p) / sqrt(p*(1-p)/n)
})
hist(sim3,nclass=7)
qqnorm((sim3))
abline(0,1)

#sim4
p <- 0.01
n <- 30
sides <- 1/p
sim4 <- replicate(N,{
  x <- sample(1:sides, n, replace=TRUE)
  z <- (mean(x==1) - p) / sqrt(p*(1-p)/n)
})
hist(sim4,nclass=7)
qqnorm((sim4))
abline(0,1)

#sim5
p <- 0.01
n <- 100
sides <- 1/p
sim5 <- replicate(N,{
  x <- sample(1:sides, n, replace=TRUE)
  z <- (mean(x==1) - p) / sqrt(p*(1-p)/n)
})
hist(sim5,nclass=7)
qqnorm((sim5))
abline(0,1)
```
    answer codes:
    ps <- c(0.5,0.5,0.01,0.01)
    ns <- c(5,30,30,100)
    library(rafalib)
    mypar(4,2)
    for(i in 1:4){
      p <- ps[i]
      sides <- 1/p
      n <- ns[i]
      zs <- replicate(10000,{
  	    x <- sample(1:sides,n,replace=TRUE)
  	    (mean(x==1) - p) / sqrt(p*(1-p)/n)
      }) 
      hist(zs,nclass=7)
      qqnorm(zs)
      abline(0,1)
    }
    
    
    - sim 3 best b/c of n and p. p too far away from 1/6 on sim4-5 see abline(0, 1)

3. In practice, we do *not* have access to entire populations.Instead, we obtain one random sample and need to reach conclusions analyzing that data. dat is an example of a typical simple dataset representing just one sample. We have 12 measurements for each of two populations:#   
 X <- filter(dat, Diet=="chow") %>% select(Bodyweight) %>% unlist
 Y <- filter(dat, Diet=="hf") %>% select(Bodyweight) %>% unlist
 
 We think of X as a random sample from the population of all mice in the control diet and Y as a random sample from the population of all mice in the high fat diet. Define the parameter mu_X as the average of the control population. We estimate this parameter with the sample average . What is the sample average?
```{r}
X <- filter(dat, Diet=="chow") %>% select(Bodyweight) %>% unlist
Y <- filter(dat, Diet=="hf") %>% select(Bodyweight) %>% unlist
X_bar <- mean(X)
X_bar
```

4. We don't know mu_x, but want to use X_bar to understand mu_x. Which of the following uses CLT to understand how well X_bar approximates mu_x? 
    - D) c

5. The result above tells us the distribution of the following random variable: Z = sqrt(12)*(X_bar - mu_x)/sigma_x. What does the CLT tell us is the mean of Z (you don't need code)?
    mean is 0
    
6. The result of 4 and 5 tell us that we know the distribution of the difference between our estimate and what we want to estimate, but don't know. However, the equation involves the population standard deviation sigma_x, which we don't know. Given what we discussed, what is your estimate of sigma_x? 
```{r}
sd(X) # sigma_x is the population sd
```

7. Use the CLT to approximate the probability that our estimate X_bar is off by more than 2 grams from mu_X.
```{r}
#mean(abs(X_bar - 0)>2)
mu_x <- 2 + X_bar 
z <- sqrt(12)*(mu_x - X_bar)/sd(X)
p1 <- 2*(1 - pnorm(z)) # or 2 * ( 1-pnorm(2/sd(X) * sqrt(12) ) )
z
p1
```
8. Now we introduce the concept of a null hypothesis. We don't know mu_x nor mu_y. We want to quantify what the data say about the possibility that the diet has no effect: mu_x = mu_y. If we use CLT, then we approximate the distribution of X_bar as normal with mean mu_x and standard deviation sigma_X / sqrt(M) and the distribution of Y_bar as normal with mean mu_y and standard deviation sigma_y / sqrt(N), with M and N the sample sizes for X and Y respectively, in this case 12. This implies that the difference Y_bar - x_bar has mean 0. We described that the standard deviation of this statistic (the standard error) is 
SE(X_bar - Y_bar) = sqrt(sigma^2_x / 12 + sigma^2_y / 12) and that we estimate the population standard deviations sigma_x and sigma_y with the sample estimates. What is the estimate of 
SE(X_bar - Y_bar) = sqrt(sigma^2_x / 12 + sigma^2_y / 12)?
```{r}
se <- sqrt(var(X) / 12 + var(Y) / 12) # or sqrt( sd(X)^2/12 + sd(Y)^2/12 )
se
```

9. So now we can compute Y_bar - X_bar as well as an estimate of this standard error and construct a t-statistic. What number is this t-statistic? 
```{r}
Y_bar <- mean(Y)
t1 <- (Y_bar - X_bar) / se # or t.test(Y,X)$stat
t1
```


For some of the following exercises you need to review the t-distribution that was discussed in the lecture. If you have not done so already,  you should review the related book chapters from our textbook which can also be found here and here.

In particular, you will need to remember that the t-distribution is centered at 0 and has one parameter: the degrees of freedom, that control the size of the tails. You will notice that if X follows a t-distribution the probability that X is smaller than an extreme value such as 3 SDs away from the mean grows with the degrees of freedom. For example, notice the difference between:

    1 - pt(3,df=3) 
    1 - pt(3,df=15)
    1 - pt(3,df=30)
    1 - pnorm(3)

As we explained, under certain assumptions, the t-statistic follows a t-distribution. Determining the degrees of freedom can sometimes be cumbersome, but the t.test function calculates it for you. One important fact to keep in mind is that the degrees of freedom are directly related to the sample size. There are various resources for learning more about degrees of freedom on the internet as well as statistics books.

10. If we apply the CLT, what is the distribution of this t-statistic?
Normal with mean 0 and standard deviation 1. correct
t-distributed with 22 degrees of freedom. - we applied CLY so no here

11. Now we are ready to compute a p-value using the CLT. What is the probability of observing a quantity as large as what we computed in 9, when the null distribution is true?  
```{r}
r_tail <- 1 - pnorm(abs(t1))
l_tail <- pnorm(-abs(t1))
p_val <- l_tail + r_tail
print(p_val)
# or Z <- ( mean(Y) - mean(X) ) / sqrt( var(X)/12 + var(Y)/12)
# p_val <- 2*( 1-pnorm(Z))
```
12. CLT provides an approximation for cases in which the sample size is large. In practice, we can't check the assumption because we only get to see 1 outcome (which you computed above). As a result, if this approximation is off, so is our p-value. As described earlier, there is another approach that does not require a large sample size, but rather that the distribution of the population is approximately normal. We don't get to see this distribution so it is again an assumption, although we can look at the distribution of the sample with qqnorm(X) and qqnorm(Y). If we are willing to assume this, then it follows that the t-statistic follows t-distribution. What is the p-value under the t-distribution approximation? Hint: use the t.test function. 
```{r}
t.test(Y,X)$p.value
```

13. With the CLT distribution, we obtained a p-value smaller than 0.05 and with the t-distribution, one that is larger. They can't both be right. What best describes the difference?

B). These are two different assumptions. The t-distribution accounts for the variability introduced by the estimation of the standard error and thus, under the null, large values are more probable under the null distribution. correct














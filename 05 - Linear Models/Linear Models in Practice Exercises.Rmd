---
title: "Linear Models in Practice Exercises"
author: "Stephen Blatti"
date: "August 21, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


In the last videos we saw how to use lm() to run a simple two group linear model, and then compared the t-value from the linear model with the t-value from a t-test with the equal variance assumption. Though the linear model in this case is equivalent to a t-test, we will soon explore more complicated designs, where the linear model is a useful extension (confounging variables, testing contrasts of terms, testing interactions, testing many terms at once, etc.)

Here we will review the mathematics on why these produce the same t-value and therefore p-value.

We already know that the numerator of the t-value in both cases is the difference between the average of the groups, so we only have to see that the denominator is the same. Of course, it makes sense that the denominator should be the same, since we are calculating the standard error of the same quanity (the difference) under the same assumptions (equal variance), but here we will show equivalence of the formula.

In the linear model, we saw how to calculate this standard error using the design matrix X and the estimate of sigma^2 from the residuals. The estimate of sigma^2 was the sum of squared residuals divided by (N - p), where N is the total number of samples and p is the number of terms (an intercept and a group indicator, so here p=2).

In the t-test, the denominator of the t-value is the standard error of the difference. The t-test formula for the standard error of the difference, if we assume equal variance in the two groups is:

SE = sqrt(var(diff))

var(diff) = (1/nx + 1/ny) ( sum { (x_i - mu_x)^2 } + sum { (y_i - mu_y)^2 } ) / (nx + ny - 2) 

Where nx is the number of samples in the first group and ny is the number of samples in the second group.

If we look carefully, the second part of this equation is the sum of squared residuals, divided by (N - 2).

So all that is left to show is that

( (X^T X)^-1 )[2,2] = (1/nx + 1/ny)

...where [2,2] indicates the 2nd row, 2nd column, with X as the design matrix of a linear model of two groups.


1. You can make a design matrix X for a two group comparison either using model.matrix or simply with:

X = cbind(rep(1,nx + ny),rep(c(0,1),c(nx, ny)))
For a comparison of two groups, where the first group has nx=5 samples, and the second group has ny=7 samples, what is the element in the 1st row and 1st column of X^T X?
```{r}
nx <- 5
ny <- 7
X = cbind(rep(1,nx + ny),rep(c(0,1),c(nx, ny)))

XtX = t(X) %*% X

XtX[ 1,1 ]
```


2. What are all the other elements of (X^t X)?
```{r}
XtX
```



Now we just need to invert the matrix to obtain (X^T X)^-1

The formula for matrix inversion for a 2x2 matrix is:

      | a b |^-1
      | c d |
      
      =  1/(ad - bc) | d  -b |
                     | -c  a |

The element of the inverse in the 2nd row and the 2nd column is the element which will be used to calculate the standard error of the second coefficient of the linear model. This is:

a / (ad - bc) 

And for our two group comparison, we saw that a = nx + ny and the b = c = d = ny. So it follows that this element is:

(nx + ny) / ((nx + ny) ny - ny ny)

which simplifies to:

(nx + ny) / (nx * ny)

which simplifies to:

(1/ny + 1/nx)




























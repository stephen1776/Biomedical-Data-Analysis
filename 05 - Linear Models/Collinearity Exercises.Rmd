---
title: "Collinearity Exercises"
author: "Stephen Blatti"
date: "August 26, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



To answer the first question, consider the following design matrices:

        A
        
        1 0 0 0
        1 0 0 0
        1 1 1 0
        1 1 0 1
        
        B
        
        1 0 0 1
        1 0 1 1
        1 1 0 0
        1 1 1 0
        
        C
        
        1 0 0 
        1 1 2 
        1 2 4 
        1 3 6 
        
        D
        
        1 0 0 0 0
        1 0 0 0 1
        1 1 0 1 0
        1 1 0 1 1
        1 0 1 1 0
        1 0 1 1 1
        
        E
        
        1 0 0 0
        1 0 1 0
        1 1 0 0
        1 1 1 1
        
        F
        
        1 0 0 1
        1 0 0 1
        1 0 1 1
        1 1 0 0
        1 1 0 0
        1 1 1 0

 
1. Which of the above design matrices does NOT have the problem of collinearity?

        A
        B
        C
        D
        E correct
        F

Explanation

You can check in R, the rank of the E matrix is equal to the number of columns, so all of the columns are independent.

m = matrix(c(1,1,1,1,0,0,1,1,0,1,0,1,0,0,0,1),4,4)

qr(m)$rank

2. Let's use the example from the lecture to visualize how there is not a single best beta-hat, when the design matrix has collinearity of columns.

An example can be made with:

sex <- factor(rep(c("female","male"),each=4))
trt <- factor(c("A","A","B","B","C","C","D","D"))

The model matrix can then be formed with:

X <- model.matrix( ~ sex + trt)

And we can see that the number of independent columns is less than the number of columns of X:

qr(X)$rank

Suppose we observe some outcome, Y. For simplicity we will use synthetic data:

Y <- 1:8

Now, we will fix the value for two beta's and optimize the remaining betas. We will fix beta_male and beta_D. And then we will find the optimal value for the remaining betas, in terms of minimizing sum((Y - X beta)^2).

The optimal value for the other betas is the one that minimizes:

sum( ( (Y - X_male* beta_male - X_D beta_D) - X_R beta_R )^2 )

Where X_male is the male column of the design matrix, X_D is the D column, and X_R has the remaining columns.

So all we need to do is redefine Y as Y* = Y - X* beta_male - X** beta_D and fit a linear model. The following line of code creates this  variable Y*, after fixing beta_male to a value 'a', and beta_D to a value, 'b':

makeYstar <- function(a,b) Y - X[,2] * a - X[,5] * b

Now we'll construct a function which, for a given value a and b, gives us back the the sum of squared residuals after fitting the other terms.

fitTheRest <- function(a,b) {
  Ystar <- makeYstar(a,b)
  Xrest <- X[,-c(2,5)]
  betarest <- solve(t(Xrest) %*% Xrest) %*% t(Xrest) %*% Ystar
  residuals <- Ystar - Xrest %*% betarest
  sum(residuals^2)
}

What is the sum of squared residuals when the male coefficient is 1 and the D coefficient is 2, and the other coefficients are fit using the linear model solution?

```{r}
sex <- factor(rep(c("female","male"),each=4))
trt <- factor(c("A","A","B","B","C","C","D","D"))

X <- model.matrix( ~ sex + trt)
qr(X)$rank
Y <- 1:8
# makeYstar <- function(a,b) Y - X[,2] * a - X[,5] * b
# sum( ( (Y - X_male* beta_male - X_D beta_D) - X_R beta_R )^2 )
makeYstar <- function(a,b) Y - X[,2] * a - X[,5] * b

fitTheRest <- function(a,b) {
  Ystar <- makeYstar(a,b)
  Xrest <- X[,-c(2,5)]
  betarest <- solve(t(Xrest) %*% Xrest) %*% t(Xrest) %*% Ystar
  residuals <- Ystar - Xrest %*% betarest
  sum(residuals^2)
}
fitTheRest(1,2)
```



It's fairly clear from just looking at the numbers, but we can also visualize the sum of squared residuals over our grid with the imagemat() function from rafalib:


library(rafalib)

## plot the pairs what are minimum

themin=min(rss)

plot(betas[which(rss==themin),])

There is clearly not a single beta which optimizes the least squares equation, due to collinearity, but an infinite line of solutions which produce an identical sum of squares values.






















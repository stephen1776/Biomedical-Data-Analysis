---
title: "Standard Errors Exercises"
author: "Stephen Blatti"
date: "August 15, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In the previous assessment, we used a Monte Carlo technique to see that the linear model coefficients are random variables when the data is a random sample. Now we will use the matrix algebra from the previous video to try to estimate the standard error of the linear model coefficients. Again, take a random sample of the father.son heights data:

    library(UsingR)
    x = father.son$fheight
    y = father.son$sheight
    n = length(y)
    N = 50
    set.seed(1)
    index = sample(n,N)
    sampledat = father.son[index,]
    x = sampledat$fheight
    y = sampledat$sheight
    betahat = lm(y~x)$coef

The formula for the standard error in the previous video was (the following two lines are not R code):

    SE(betahat) = sqrt(var(betahat))
    var(betahat) = sigma^2 (X^T X)^-1

This is also listed in the standard error book page (http://genomicsclass.github.io/book/pages/standard_errors.html).

We will estimate or calculate each part of this equation and then combine them.

First, we want to estimate sigma^2, the variance of Y. As we have seen in the previous unit, the random part of Y is only coming from epsilon, because we assume X*beta is fixed. So we can try to estimate the variance of the epsilons from the residuals, the Yi minus the fitted values from the linear model.

1. Note that the fitted values (Y-hat) from a linear model can be obtained with:

fit = lm(y ~ x)

fit$fitted.values
What is the sum of the squared residuals (where residuals are given by r_i = Y_i - Y-hat_i)?

```{r}
library(UsingR)
x = father.son$fheight
y = father.son$sheight
n = length(y)
N = 50
set.seed(1)
index = sample(n,N)
sampledat = father.son[index,]
x = sampledat$fheight
y = sampledat$sheight
betahat = lm(y~x)$coef
fit = lm(y ~ x)
#Y_hat <- fit$fitted.values

SSR <- sum((y - fit$fitted.values)^2)
SSR

```

2. Our estimate of sigma^2 will be the sum of squared residuals divided by (N - p), the sample size minus the number of terms in the model. Since we have a sample of 50 and 2 terms in the model (an intercept and a slope), our estimate of sigma^2 will be the sum of squared residuals divided by 48. Save this to a variable 'sigma2':

sigma2 = SSR / 48

where SSR is the answer to the previous question.


Form the design matrix X (note: use a capital X!). This can be done by combining a column of 1's with a column of 'x' the father's heights.

X = cbind(rep(1,N), x)
Now calculate (X^T X)^-1, the inverse of X transpose times X. Use the solve() function for the inverse and t() for the transpose. What is the element in the first row, first column?

```{r}
p <- 2
sigma2 <- SSR / (N - p)
X = cbind(rep(1,N), x)
solve(t(X) %*% X)

```

3. Now we are one step away from the standard error of beta-hat. Take the diagonals from the (X^T X)^-1 matrix above, using the diag() function. Now multiply our estimate of sigma^2 and the diagonals of this matrix. This is the estimated variance of beta-hat, so take the square root of this. You should end up with two numbers, the standard error for the intercept and the standard error for the slope.
What is the standard error for the slope?

```{r}
sqrt(sigma2 %*% diag( solve(t(X) %*% X) ) )
```

      Compare this value with the value you estimated using Monte Carlo in the previous assessment. It will not be the same, because we are only estimating the standard error given a particular sample of 50 (which we obtained with set.seed(1)).
      
      Note that the standard error estimate is also printed in the second column of:

```{r}
summary(fit)
```


We have shown how we can obtain standard errors for our estimates. But, as we learned in PH525.1x to perform inference we need to know the distribution of these random variables. The reason we went through the effort of computing the standard errors is because the CLT applies in linear models. If  N is large enough, then the LSE will be normally distributed with mean beta and standard errors as described in our videos. For small samples, if the error term is normally distributed then the betahat-beta follow a t-distribution. Proving this mathematically is rather advanced, but the results are extremely useful as it is how we construct p-values and confidence intervals in the context of linear models.


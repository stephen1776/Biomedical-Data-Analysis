---
title: "Interactions Exercises"
author: "Stephen Blatti"
date: "August 25, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
url <- "https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/spider_wolff_gorb_2013.csv"
filename <- "spider_wolff_gorb_2013.csv"
library(downloader)
if (!file.exists(filename)) download(url, filename)
spider <- read.csv(filename, skip=1)
```
Suppose that we notice that the within-group variances for the groups with smaller frictional coefficients are generally smaller, and so we try to apply a transformation to the frictional coefficients to make the within-group variances more constant.

Add a new variable log2friction to the spider dataframe:
```{r}
spider$log2friction <- log2(spider$friction)
```
The 'Y' values now look like:
```{r}
boxplot(log2friction ~ type*leg, data=spider)
```
Run a linear model of log2friction with type, leg and interactions between type and leg.
```{r}
lm1 <- lm(log2friction ~ type*leg, data = spider)
# or lm1 <- lm(log2friction ~ type + leg + type:leg, data=spider)
```


1. What is the t-value for the interaction of type push and leg L4? If this t-value is sufficiently large, we would reject the null hypothesis that the push vs pull effect on log2(friction) is the same in L4 as in L1.

```{r}
summary(lm1)
```
2. What is the F-value for all of the type:leg interaction terms, in an analysis of variance? If this value is sufficiently large, we would reject the null hypothesis that the push vs pull effect on log2(friction) is the same for all leg pairs.

```{r}
anova(lm1)
```

3. What is the L2 vs L1 estimate in log2friction for the pull samples?
```{r}
# just read from quest 1 outout of leg2 or
library(contrast)
contrast(lm1, list(type="pull",leg="L2"), list(type="pull",leg="L1"))
coef(lm1)["legL2"]
```

4. What is the L2 vs L1 estimate in log2friction for the push samples? Remember, because of the interaction terms, this is not the same as the L2 vs L1 difference for the pull samples. If you're not sure use the contrast() function. Another hint: consider the arrows plot for the model with interactions.
```{r}
contrast(lm1, list(type="push",leg="L2"), list(type="push",leg="L1"))
coef(lm1)["legL2"] + coef(lm1)["typepush:legL2"]
```



Note that taking the log2 of a Y value and then performing a linear model has a meaningful effect on the coefficients. If we have,

log2(Y_1) = beta_0

and

log2(Y_2) = beta_0 + beta_1

Then Y_2/Y_1 = 2^(beta_0 + beta_1) / 2^(beta_0)

= 2^beta_1

So beta_1 represents a log2 fold change of Y_2 over Y_1. If beta_1 = 1, then Y_2 is 2 times Y_1. If beta_1 = -1, then Y_2 is half of Y_1, etc.

In the video we briefly mentioned the analysis of variance (or ANOVA, performed in R using the anova() function), which allows us to test whether a number of coefficients are equal to zero, by comparing a linear model including these terms to a linear model where these terms are set to 0.

The book page for this section has a section, "Testing all differences of differences", which explains the ANOVA concept and the F-test in some more detail. You can read over that section before or after the following question.

In this last question, we will use Monte Carlo techniques to observe the distribution of the ANOVA's "F-value" under the null hypothesis, that there are no differences between groups.

Suppose we have 4 groups, and 10 samples per group, so 40 samples overall:

N <- 40
p <- 4
group <- factor(rep(1:p,each=N/p))
X <- model.matrix(~ group)

We will show here how to calculate the "F-value", and then we will use random number to observe the distribution of the F-value under the null hypothesis.

The F-value is the mean sum of squares explained by the terms of interest (in our case, the 'group' terms) divided by the mean sum of squares of the residuals of a model including the terms of interest. So it is the explanatory power of the terms divided by the leftover variance.

Intuitively, if this number is large, it means that the group variable explains a lot of the variance in the data, compared to the amount of variance left in the data after using group information. We will calculate these values exactly here:

First generate some random, null data, where the mean is the same for all groups:

Y <- rnorm(N,mean=42,7)

The base model we wil compare against is simply Y-hat = mean(Y), which we will call mu0, and the initial sum of squares is the Y values minus mu0:

mu0 <- mean(Y)
initial.ss <- sum((Y - mu0)^2)

We then need to calculate the fitted values for each group, which is simply the mean of each group, and the residuals from this model, which we will call "after.group.ss" for the sum of squares after using the group information:

s <- split(Y, group)
after.group.ss <- sum(sapply(s, function(x) sum((x - mean(x))^2)))

Then the explanatory power of the group variable is the initial sum of squares minus the residual sum of squares:

(group.ss <- initial.ss - after.group.ss)

We calculate the mean of these values, but we divide by terms which remove the number of fitted parameters. For the group sum of squares, this is number of parameters used to fit the groups (3, because the intercept is in the initial model). For the after group sum of squares, this is the number of samples minus the number of parameters total (So N - 4, including the intercept).

group.ms <- group.ss / (p - 1)
after.group.ms <- after.group.ss / (N - p)

The F-value is simply the ratio of these mean sum of squares.

f.value <- group.ms / after.group.ms

What's the point of all these calculations? The point is that, after following these steps, the exact distribution of the F-value has a nice mathematical formula under the null hypothesis. We will see this below.

5. Set the seed to 1, set.seed(1) then calculate the F-value for 1000 random versions of Y. What is the mean of these F-values?
```{r}
set.seed(1)
N <- 40
p <- 4
B <- 1000
F_Vals <- replicate(B,{
  group <- factor(rep(1:p,each=N/p))
  X <- model.matrix(~ group)
  Y <- rnorm(N,mean=42,7)
  mu0 <- mean(Y)
  initial.ss <- sum((Y - mu0)^2)
  s <- split(Y, group)
  after.group.ss <- sum(sapply(s, function(x) sum((x - mean(x))^2)))
  group.ss <- initial.ss - after.group.ss
  group.ms <- group.ss / (p - 1)
  after.group.ms <- after.group.ss / (N - p)
  f.value <- group.ms / after.group.ms
})
mean(F_Vals)


```
Plot the distribution of the 1000 F-values:
hist(Fs, col="grey", border="white", breaks=50, freq=FALSE)

Overlay the theoretical F-distribution, with parameters df1=p - 1, df2=N - p.
xs <- seq(from=0,to=6,length=100)
lines(xs, df(xs, df1 = p - 1, df2 = N - p), col="red")
This is the distribution which is used to calculate the p-values for the ANOVA table produced by anova(). 





































